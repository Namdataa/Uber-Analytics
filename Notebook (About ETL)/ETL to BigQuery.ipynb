{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9285ae",
   "metadata": {},
   "source": [
    "## 1. Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "def load_data_from_api():\n",
    "    \"\"\"\n",
    "    Load data from a public CSV hosted on Google Cloud Storage\n",
    "    \"\"\"\n",
    "    url = 'https://storage.googleapis.com/uber-bucket-nam6mui/uber_data.csv'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Kiểm tra xem yêu cầu có thành công không\n",
    "    if response.status_code == 200:\n",
    "        df = pd.read_csv(io.StringIO(response.text), sep=',')\n",
    "        return df\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c4e3b",
   "metadata": {},
   "source": [
    "# 2. Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5fe11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Template code for a transformer block.\n",
    "\n",
    "    Add more parameters to this function if this block has multiple parent blocks.\n",
    "    There should be one parameter for each output variable from each parent block.\n",
    "\n",
    "    Args:\n",
    "        data: The output from the upstream parent block\n",
    "        args: The output from any additional upstream blocks (if applicable)\n",
    "\n",
    "    Returns:\n",
    "        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n",
    "    \"\"\"\n",
    "    # Specify your transformation logic here\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "\n",
    "    datetime_dim = df[['tpep_pickup_datetime','tpep_dropoff_datetime']].drop_duplicates().reset_index(drop=True)\n",
    "    datetime_dim['pick_hour'] = datetime_dim['tpep_pickup_datetime'].dt.hour\n",
    "    datetime_dim['pick_day'] = datetime_dim['tpep_pickup_datetime'].dt.day\n",
    "    datetime_dim['pick_month'] = datetime_dim['tpep_pickup_datetime'].dt.month\n",
    "    datetime_dim['pick_year'] = datetime_dim['tpep_pickup_datetime'].dt.year\n",
    "    datetime_dim['pick_weekday'] = datetime_dim['tpep_pickup_datetime'].dt.weekday\n",
    "\n",
    "    datetime_dim['drop_hour'] = datetime_dim['tpep_dropoff_datetime'].dt.hour\n",
    "    datetime_dim['drop_day'] = datetime_dim['tpep_dropoff_datetime'].dt.day\n",
    "    datetime_dim['drop_month'] = datetime_dim['tpep_dropoff_datetime'].dt.month\n",
    "    datetime_dim['drop_year'] = datetime_dim['tpep_dropoff_datetime'].dt.year\n",
    "    datetime_dim['drop_weekday'] = datetime_dim['tpep_dropoff_datetime'].dt.weekday\n",
    "\n",
    "    datetime_dim['datetime_id'] = datetime_dim.index\n",
    "    datetime_dim = datetime_dim[['datetime_id', 'tpep_pickup_datetime', 'pick_hour', 'pick_day', 'pick_month', 'pick_year', 'pick_weekday',\n",
    "                             'tpep_dropoff_datetime', 'drop_hour', 'drop_day', 'drop_month', 'drop_year', 'drop_weekday']]\n",
    "\n",
    "    passenger_count_dim = df[['passenger_count']].drop_duplicates().reset_index(drop=True)\n",
    "    passenger_count_dim['passenger_count_id'] = passenger_count_dim.index\n",
    "    passenger_count_dim = passenger_count_dim[['passenger_count_id','passenger_count']]\n",
    "\n",
    "    trip_distance_dim = df[['trip_distance']].drop_duplicates().reset_index(drop=True)\n",
    "    trip_distance_dim['trip_distance_id'] = trip_distance_dim.index\n",
    "    trip_distance_dim = trip_distance_dim[['trip_distance_id','trip_distance']]\n",
    "    rate_code_type = {\n",
    "        1:\"Standard rate\",\n",
    "        2:\"JFK\",\n",
    "        3:\"Newark\",\n",
    "        4:\"Nassau or Westchester\",\n",
    "        5:\"Negotiated fare\",\n",
    "        6:\"Group ride\"\n",
    "    }\n",
    "\n",
    "    rate_code_dim = df[['RatecodeID']].drop_duplicates().reset_index(drop=True)\n",
    "    rate_code_dim['rate_code_id'] = rate_code_dim.index\n",
    "    rate_code_dim['rate_code_name'] = rate_code_dim['RatecodeID'].map(rate_code_type)\n",
    "    rate_code_dim = rate_code_dim[['rate_code_id','RatecodeID','rate_code_name']]\n",
    "\n",
    "\n",
    "    pickup_location_dim = df[['pickup_longitude', 'pickup_latitude']].drop_duplicates().reset_index(drop=True)\n",
    "    pickup_location_dim['pickup_location_id'] = pickup_location_dim.index\n",
    "    pickup_location_dim = pickup_location_dim[['pickup_location_id','pickup_latitude','pickup_longitude']] \n",
    "\n",
    "\n",
    "    dropoff_location_dim = df[['dropoff_longitude', 'dropoff_latitude']].drop_duplicates().reset_index(drop=True)\n",
    "    dropoff_location_dim['dropoff_location_id'] = dropoff_location_dim.index\n",
    "    dropoff_location_dim = dropoff_location_dim[['dropoff_location_id','dropoff_latitude','dropoff_longitude']]\n",
    "\n",
    "    payment_type_name = {\n",
    "        1:\"Credit card\",\n",
    "        2:\"Cash\",\n",
    "        3:\"No charge\",\n",
    "        4:\"Dispute\",\n",
    "        5:\"Unknown\",\n",
    "        6:\"Voided trip\"\n",
    "    }\n",
    "    payment_type_dim = df[['payment_type']].drop_duplicates().reset_index(drop=True)\n",
    "    payment_type_dim['payment_type_id'] = payment_type_dim.index\n",
    "    payment_type_dim['payment_type_name'] = payment_type_dim['payment_type'].map(payment_type_name)\n",
    "    payment_type_dim = payment_type_dim[['payment_type_id','payment_type','payment_type_name']]\n",
    "\n",
    "    fact_table = df.merge(passenger_count_dim, on='passenger_count') \\\n",
    "             .merge(trip_distance_dim, on='trip_distance') \\\n",
    "             .merge(rate_code_dim, on='RatecodeID') \\\n",
    "             .merge(pickup_location_dim, on=['pickup_longitude', 'pickup_latitude']) \\\n",
    "             .merge(dropoff_location_dim, on=['dropoff_longitude', 'dropoff_latitude'])\\\n",
    "             .merge(datetime_dim, on=['tpep_pickup_datetime','tpep_dropoff_datetime']) \\\n",
    "             .merge(payment_type_dim, on='payment_type') \\\n",
    "             [['VendorID', 'datetime_id', 'passenger_count_id',\n",
    "               'trip_distance_id', 'rate_code_id', 'store_and_fwd_flag', 'pickup_location_id', 'dropoff_location_id',\n",
    "               'payment_type_id', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
    "               'improvement_surcharge', 'total_amount']]\n",
    "\n",
    "    return {\"datetime_dim\":datetime_dim.to_dict(orient=\"dict\"),\n",
    "    \"passenger_count_dim\":passenger_count_dim.to_dict(orient=\"dict\"),\n",
    "    \"trip_distance_dim\":trip_distance_dim.to_dict(orient=\"dict\"),\n",
    "    \"rate_code_dim\":rate_code_dim.to_dict(orient=\"dict\"),\n",
    "    \"pickup_location_dim\":pickup_location_dim.to_dict(orient=\"dict\"),\n",
    "    \"dropoff_location_dim\":dropoff_location_dim.to_dict(orient=\"dict\"),\n",
    "    \"payment_type_dim\":payment_type_dim.to_dict(orient=\"dict\"),\n",
    "    \"fact_table\":fact_table.to_dict(orient=\"dict\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a715e",
   "metadata": {},
   "source": [
    "## 3. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7968d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "# Cấu hình\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    'api_connect_bigquery.json' # File Key API connect BigQuery\n",
    ")\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "def export_data_to_bigquery(data_dict: dict, project_id: str, dataset_id: str) -> None:\n",
    "    \"\"\"\n",
    "    Upload multiple tables (DataFrame) lên BigQuery từ dict.\n",
    "    data_dict: dict, key = tên bảng, value = dict dữ liệu (định dạng to_dict của DataFrame).\n",
    "    project_id: str, project BigQuery của bạn\n",
    "    dataset_id: str, dataset trong project BigQuery của bạn\n",
    "    \"\"\"\n",
    "\n",
    "    for table_name, data_json in data_dict.items():\n",
    "        # Chuyển dict JSON thành DataFrame\n",
    "        df = pd.DataFrame.from_dict(data_json)\n",
    "\n",
    "        # Định nghĩa table_id đầy đủ\n",
    "        table_id = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "\n",
    "        # Cấu hình job: ghi đè bảng nếu đã tồn tại\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "\n",
    "        # Upload DataFrame lên BigQuery\n",
    "        load_job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        load_job.result()  # Đợi job hoàn tất\n",
    "\n",
    "        print(f\"✅ Đã upload bảng {table_name} lên BigQuery: {table_id}\")\n",
    "        \n",
    "export_data_to_bigquery(df, project_id=\"test-uberbi\", dataset_id=\"uber_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
